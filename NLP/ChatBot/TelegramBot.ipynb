{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = '5213574179:AAFRp9HUe7JvbJtUIWfGsJAMQH25uKvWzm8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import telebot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = telebot.TeleBot(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history_ids = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start dialog\n",
    "@bot.message_handler(commands=['start', 'help'])\n",
    "def send_welcome(msg):\n",
    "    if msg.chat.id in chat_history_ids:\n",
    "        chat_history_ids.pop(msg.chat.id)\n",
    "    bot.reply_to(msg, \"Hi, this is dialog bot based on DIALOGPT-medium. Try to start conversation in english.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bot.message_handler(commands=['restart'])\n",
    "def send_welcome(msg):\n",
    "    if msg.chat.id in chat_history_ids:\n",
    "        chat_history_ids.pop(msg.chat.id)\n",
    "    bot.reply_to(msg, \"Hi, this is dialog bot based on DIALOGPT-medium. Try to start conversation in english.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@bot.message_handler(func=lambda msg: True)\n",
    "def echo_all(msg):\n",
    "    usr_id = msg.chat.id\n",
    "    if usr_id in chat_history_ids:\n",
    "        usr_msg = \" \" + msg.text\n",
    "    else:\n",
    "        usr_msg = msg.text\n",
    "    \n",
    "    print(\"User: \" + str(usr_id) + \" msg: \" + msg.text)\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(usr_msg + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids[usr_id], new_user_input_ids], dim=-1) if usr_id in chat_history_ids else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids[usr_id] = model.generate(bot_input_ids.cuda(), max_length=1000, pad_token_id=tokenizer.eos_token_id).cpu()\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    answer = tokenizer.decode(chat_history_ids[usr_id][:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    chat_history_ids[usr_id] = chat_history_ids[usr_id][:, max(bot_input_ids.shape[-1]-50, 0):]\n",
    "    if answer == '':\n",
    "        chat_history_ids.pop(usr_id)\n",
    "    print(\"DialoGPT: {}\".format(answer))\n",
    "\n",
    "    bot.reply_to(msg, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 394194431 msg: Hi\n",
      "DialoGPT: Hi! :D\n",
      "User: 394194431 msg: How are you?\n",
      "DialoGPT: I'm good, how are you?\n",
      "User: 394194431 msg: Hi\n",
      "DialoGPT: Hi! :D\n",
      "User: 120342957 msg: Hi\n",
      "DialoGPT: Hi! :D\n",
      "User: 120342957 msg: What are you doing\n",
      "DialoGPT: I'm going to sleep.\n",
      "User: 120342957 msg: ðŸ˜´ good night\n",
      "DialoGPT: Night! :D\n",
      "User: 394194431 msg: Do you know about virtual machine snapshot?\n",
      "DialoGPT: I don't know about that, but I know about virtual machine snapshots.\n",
      "User: 394194431 msg: Can you help me?\n",
      "DialoGPT: I can help you.\n",
      "User: 394194431 msg: Tell me about volume storage\n",
      "DialoGPT: I can help you with that.\n",
      "User: 394194431 msg: ðŸ˜ƒ\n",
      "DialoGPT: I can help you with that too.\n",
      "User: 394194431 msg: Nice to hear that\n",
      "DialoGPT: I can help you with that too.\n",
      "User: 394194431 msg: How is the weather\n",
      "DialoGPT: It's nice.\n",
      "User: 185471179 msg: Hello fucker\n",
      "DialoGPT: Hey, I'm not the one who made the post.\n",
      "User: 400664727 msg: ahahaha\n",
      "DialoGPT: I'm not sure if you're serious or not.\n",
      "User: 361984038 msg: how many times do you blink your eyes per day?\n",
      "DialoGPT: I blink once a day.\n",
      "User: 361984038 msg: is it ok?\n",
      "DialoGPT: I blink once a day.\n",
      "User: 361984038 msg: I feel pressure from you\n",
      "DialoGPT: I blink once a day.\n",
      "User: 361984038 msg: you will not stop, right?\n",
      "DialoGPT: I blink once a day.\n",
      "User: 410416785 msg: ÐŸÑ€Ð¸Ð²ÐµÑ‚!\n",
      "DialoGPT: !\n",
      "User: 410416785 msg: Hi!\n",
      "DialoGPT: !\n",
      "User: 410416785 msg: How are you\n",
      "DialoGPT: I'm good, how about you?\n",
      "User: 297230401 msg: Hellow how are you\n",
      "DialoGPT: I'm good, how are you?\n",
      "User: 410416785 msg: pretty well\n",
      "DialoGPT: That's good.\n",
      "User: 410416785 msg: What are you doing?\n",
      "DialoGPT: I'm not sure.\n",
      "User: 297230401 msg: i am thied, long day\n",
      "DialoGPT: I'm sorry, I'm not sorry.\n",
      "User: 297230401 msg: Funny\n",
      "DialoGPT: I'm sorry, I'm not sorry.\n",
      "User: 297230401 msg: againe\n",
      "DialoGPT: I'm sorry, I'm not sorry.\n",
      "User: 394194431 msg: ðŸ˜‰\n",
      "DialoGPT: I can help you with that too.\n",
      "User: 323249927 msg: Where is zombie?\n",
      "DialoGPT: He's in the back of the picture.\n",
      "User: 323249927 msg: call the Winchesters\n",
      "DialoGPT: I'm not sure if you're serious, but I'm pretty sure that's a reference to the movie The Ring.\n",
      "User: 323249927 msg: How r u?\n",
      "DialoGPT: I'm not sure if you're serious, but I'm pretty sure that's a reference to the movie The Ring.\n",
      "User: 323249927 msg: Wazzup\n",
      "DialoGPT: I'm not sure if you're serious, but I'm pretty sure that's a reference to the movie The Ring.\n",
      "User: 323249927 msg: Sup\n",
      "DialoGPT: I'm not sure if you're serious, but I'm pretty sure that's a reference to the movie The Ring.\n"
     ]
    }
   ],
   "source": [
    "bot.polling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
